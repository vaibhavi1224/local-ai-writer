# âœ… `README.md`

# âœï¸ Local AI Writer â€“ Using Ollama & LLaMA3

This project is a lightweight AI writing app that runs **100% locally** using the LLaMA3 model and **Ollama** for inference. It helps generate blog introductions from a given topicâ€”**without using OpenAI or cloud APIs**.


## âœ… App Type
**AI Writer** â€“ Generates blog intros from a user-given topic.


## ğŸ§  Model Used
- **Model**: `llama3`  
- **Runtime**: Local via [Ollama](https://ollama.com)


## ğŸš€ Features

- Topic input prompt
- Local inference using LLaMA3 (no external API)
- Temperature control slider
- Output logging (saved in `logs/outputs.txt`)
- Simple Streamlit-based UI
- Lightweight and fast


## ğŸ›  How to Run Locally

### 1. Install [Ollama](https://ollama.com/download)
```bash
# Then pull the llama3 model:
ollama run llama3
````

### 2. Install Python dependencies

```bash
pip install -r requirements.txt
```

### 3. Start the app

```bash
streamlit run app.py
```

---

## ğŸ“ Project Structure

```
local-ai-writer/
â”œâ”€â”€ app.py                  # Main Streamlit app
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ outputs.txt         # Logged responses
â””â”€â”€ README.md               # Project info
```

---

## ğŸ“ Notes

* All model inference happens **locally** using Ollama.
* No keys or cloud dependencies required.
* Can be easily extended for Rephraser, Explainer, or Chatbot use-cases.
